{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwBwbv7u3j4m",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-huggingface langchain_community langchainhub langchain-chroma bs4\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_core.tools import tool\n",
        "from transformers import pipeline\n",
        "from langchain import hub\n",
        "import getpass\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_api_key\""
      ],
      "metadata": {
        "id": "QtCWs03c35y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id, temperature=0.5, verbose=True\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MDaIpkQz3836"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8PQWdNVwcOW"
      },
      "source": [
        "# SQL + LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji77O5oeFJnr"
      },
      "source": [
        "Escolher database a ser utilizada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mPI3Jslww1uz"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///chinook.db\")\n",
        "db.get_usable_table_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjYnLrfWFXL_"
      },
      "source": [
        "Criar chain que transforma pergunta em uma query SQL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hHItGFWyXGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a1f96c51-6886-4904-e6f2-0ca83a18fc9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT COUNT(*) FROM employees;'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from langchain.chains import create_sql_query_chain\n",
        "\n",
        "chain = create_sql_query_chain(llm, db)\n",
        "response = chain.invoke({\"question\": \"How many employees are there\"})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06uuvYPxFvI-"
      },
      "source": [
        "Consultar a database com a query gerada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_KZYQa0QyZf5",
        "outputId": "eb49fd52-a18f-44e4-de96-c22de9a41daa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[(8,)]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "db.run(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWxl54tKF3-S"
      },
      "source": [
        "Prompt utilizado pela chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ShH4V5uybOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2ea2ba-fc06-4395-9c75-dd0f23f1ec6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\n",
            "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\n",
            "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
            "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
            "Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: Question here\n",
            "SQLQuery: SQL Query to run\n",
            "SQLResult: Result of the SQLQuery\n",
            "Answer: Final answer here\n",
            "\n",
            "Only use the following tables:\n",
            "\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\n",
            "\n",
            "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "chain.get_prompts()[0].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTa-zaRgF7dH"
      },
      "source": [
        "Criar tool para consultar a base de dados a partir da query gerada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ5_xMCCycID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a8b0234e-1a57-4b8a-c76b-d14558ee42ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[(8,)]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
        "\n",
        "execute_query = QuerySQLDataBaseTool(db=db)\n",
        "write_query = create_sql_query_chain(llm, db)\n",
        "chain = write_query | execute_query\n",
        "chain.invoke({\"question\": \"How many employees are there\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVZBAFKoHQ5J"
      },
      "source": [
        "Criar chain única para converter a pergunta para SQL, executar a query e devolver resposta em linguagem natural:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAJFiN9YyfpR"
      },
      "outputs": [],
      "source": [
        "answer_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Result: {result}\n",
        "Answer: \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_prompt | llm | StrOutputParser()\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(query=write_query).assign(\n",
        "        result=itemgetter(\"query\") | execute_query\n",
        "    )\n",
        "    | answer\n",
        ")\n",
        "chain.invoke({\"question\": \"How many employees are there\"})"
      ],
      "metadata": {
        "id": "6VSUzDW5GrzM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fa36368e-9e19-41d0-a865-ce622be72b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8 employees.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNBd6VmGyp0r"
      },
      "source": [
        "# GraphCypherQAChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYexapS9I4ip"
      },
      "source": [
        "**Instalações e importações necessárias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8Nwgim7O1nWy"
      },
      "outputs": [],
      "source": [
        "!pip -q install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eEBzRq9y1Ky"
      },
      "outputs": [],
      "source": [
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9SdYFwHI9RI"
      },
      "source": [
        "Conexão com o grafo e criação do prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14o6yXSXIJCQ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.graphs import Neo4jGraph\n",
        "graph = Neo4jGraph(\n",
        "    url=\"bolt://44.193.17.131:7687\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"recruit-envelope-runoffs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph.schema)"
      ],
      "metadata": {
        "id": "UikVYaqJU0SJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esVOB43bJDJk"
      },
      "source": [
        "Criação de chain para consultar grafo:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
        "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
        "Convert the user's question based on the schema.\n",
        "\n",
        "Schema: {schema}\n",
        "Question: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CmW_nHqEH11t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5JDlkkr1fdS"
      },
      "outputs": [],
      "source": [
        "cypher_generation_prompt = PromptTemplate(\n",
        "    template=CYPHER_GENERATION_TEMPLATE,\n",
        "    input_variables=[\"schema\", \"question\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cypher_chain = GraphCypherQAChain.from_llm(\n",
        "    llm,\n",
        "    graph=graph,\n",
        "    cypher_prompt=cypher_generation_prompt,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "OY0SOf6SH7mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feqs6mizJSxy"
      },
      "source": [
        "**Exemplos de funcionamento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXEsyaevIXU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c408b7df-85f5-4036-a114-4012ce0975fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mcypher\n",
            "MATCH (a:Actor)-[:ACTED_IN]->(m:Movie)\n",
            "WHERE m.title = 'Toy Story' AND a.name = 'Tom Hanks'\n",
            "RETURN a.name AS actorName, m.title AS movieTitle\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'actorName': 'Tom Hanks', 'movieTitle': 'Toy Story'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"Who voices the character 'Woody' in Toy Story?\",\n",
              " 'result': \" Tom Hanks voices the character 'Woody' in Toy Story.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "cypher_chain.invoke({\"query\": \"Who voices the character 'Woody' in Toy Story?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGqksR8dJUvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277fab31-477e-4aa6-9c83-3c3c5892665c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "MATCH (p:Person)-[:ACTED_IN]->(m:Movie)\n",
            "WHERE p.name = 'Tom Hanks'\n",
            "RETURN count(*) as numMovies\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'numMovies': 38}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"How many movies is 'Tom Hanks' in?\",\n",
              " 'result': ' Tom Hanks is in 38 movies.'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "cypher_chain.invoke({\"query\": \"How many movies is 'Tom Hanks' in?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "eGDXbTUhUdY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "CKSHECcHPNhV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "   web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "   bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "WRzogGzbR4cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "id": "xI7c8zbYR5Fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8fc856-e78a-4580-b2dc-151f07dc9be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "   chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "xu88t_eMSTKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0]"
      ],
      "metadata": {
        "id": "_4PGi-fhQ6zQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cc8320-44bf-430f-d992-d5b1d003c388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
      ],
      "metadata": {
        "id": "TJJgMQ6hu91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "YWBQ6RGUveD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "prompt.pretty_print()"
      ],
      "metadata": {
        "id": "hHaWpHRrv7nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b988a0fb-d8a5-4f8b-b8ad-5058c2e97cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
            "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "   return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "ln2VOZ4iv_dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "mlokIh-OwBRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "4MD6brSewVvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "c4f6baaf-26ce-472b-84f3-6557092b754b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nTask decomposition is a process where a complex problem is broken down into smaller, manageable tasks. This can be done through language models with simple prompting, task-specific instructions, or human inputs. For example, a user might ask a language model to \"Write a story outline for a novel.\" The language model would then decompose this task into smaller steps, such as \"Create a list of main characters,\" \"Determine the setting,\" etc. This tree structure allows for efficient and effective problem-solving.\\n\\nHowever, it\\'s important to note that task decomposition can be challenging for language models, especially over long-term planning. They may struggle to adjust plans when faced with unexpected errors and may not be as robust as humans in learning from trial and error. Despite these challenges, task decomposition is a crucial aspect of problem-solving and is essential for the efficient execution of complex tasks.\\n\\nReference(s):\\n- Yao, S., Li, Y., & Wang, Z. (2023). Tree of Thoughts: A Large-scale Multimodal Reasoning System. arXiv preprint arXiv:2303.16572.\\n- Shen, W., Zhang, J., & Wang, Y. (2023). HuggingGPT: A Large-scale Language Model for Instruction Following. arXiv preprint arXiv:2303.16573.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Search"
      ],
      "metadata": {
        "id": "28GrR-1KUeo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  rank_bm25 > /dev/null langchain-ai21 faiss-cpu"
      ],
      "metadata": {
        "id": "GvXxj6j76Puq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import os"
      ],
      "metadata": {
        "id": "VpEtHuMf6mUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_list = [\n",
        "    \"To know the direction, you have to look right\",\n",
        "    \"This is correct\",\n",
        "    \"You are right\",\n",
        "    \"Right after the meeting, we can have lunch\",\n",
        "     \"Turn left at the next intersection\"\n",
        "]"
      ],
      "metadata": {
        "id": "4dFydD0hsT7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_retriever = BM25Retriever.from_texts(doc_list)\n",
        "bm25_retriever.k = 2"
      ],
      "metadata": {
        "id": "9xYtswRg7zBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_retriever.invoke(\"right\")"
      ],
      "metadata": {
        "id": "6SAbuAn770nH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95915a0c-f676-4f27-c839-93deaa676a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='You are right'),\n",
              " Document(page_content='To know the direction, you have to look right')]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_vectorstore = FAISS.from_texts(doc_list, embeddings)\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "PklwfxCCjt5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever.invoke(\"right\")"
      ],
      "metadata": {
        "id": "VdjP35nGj9DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bb8c58-a3c0-4109-b67a-330d93ed5722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='This is correct'),\n",
              " Document(page_content='You are right')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])"
      ],
      "metadata": {
        "id": "mPzG12QHkox6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = ensemble_retriever.invoke(\"right\")\n",
        "docs"
      ],
      "metadata": {
        "id": "tBmahOPBkqxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba9c56b-76f8-4223-ff77-761211bf3b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='You are right'),\n",
              " Document(page_content='This is correct'),\n",
              " Document(page_content='To know the direction, you have to look right')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}